import ollama
import database


def query_llama_with_context(prompt):
    """
    Queries the LLM with added context from tasks and notes.
    """
    # Fetch tasks and notes from the database
    tasks = database.get_tasks()
    notes = database.get_notes()

    # Add context about tasks and notes
    context = "Here are your tasks: "
    context += ", ".join(tasks) if tasks else "You have no tasks."
    context += " And here are your notes: "
    context += ", ".join(notes) if notes else "You have no notes."

    # Combine context with user prompt
    full_prompt = f"{context}\n\n{prompt}"
    
    # Query the LLM
    response = query_llama(full_prompt)  # Use existing query_llama method
    return response

def query_llama_with_context_stream(prompt):
    """
    basicaly the main streaming function of llm response
    """
    # Fetch tasks and notes from the database
    tasks = database.get_tasks()
    notes = database.get_notes()

    # Add context about tasks and notes
    context = "Here are your tasks: "
    context += ", ".join(tasks) if tasks else "You have no tasks."
    context += " And here are your notes: "
    context += ", ".join(notes) if notes else "You have no notes."

    # Combine context with user prompt
    full_prompt = f"{context}\n\n{prompt}"

    # Stream response from the LLM
    stream = ollama.chat(
        model="llama3",
        messages=[{"role": "user", "content": full_prompt}],
        stream=True,
    )
    return stream  # Return the generator for streaming


def query_llama(prompt):
    """
    Query the LLaMA3 model using the Ollama API.
    
    Args:
        prompt (str): The question or task description to be sent to the model.
    
    Returns:
        str: The response generated by the model.
    
    Raises:
        ValueError: If the prompt is empty.
        Exception: If there is an error during API interaction.
    """
    if not prompt:
        raise ValueError("Prompt cannot be empty.")

    try:
        # Call the model and get the response
        stream = ollama.chat(
            model='llama3',
            messages=[{'role': 'user', 'content': prompt}],
            stream=True,
        )
        response = ""
        for chunk in stream:
            response += chunk['message']['content']
        
        return response

    except KeyError as e:
        raise Exception(f"Unexpected response format from the model: {str(e)}") from e
    except ollama.APIError as e:
        raise Exception(f"API error while communicating with the model: {str(e)}") from e
    except Exception as e:
        raise Exception(f"An unexpected error occurred: {str(e)}") from e

